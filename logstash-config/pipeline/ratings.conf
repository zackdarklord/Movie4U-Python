# Configuration pour l'ingestion de ratings.dat

input {
  file {
    path => "/usr/share/logstash/pipeline/data_input/ratings.dat"  # Chemin du fichier source
    start_position => "beginning"  # Commencer depuis le début du fichier
    sincedb_path => "/dev/null"  # Ignorer le fichier de suivi de position
    ignore_older => 0  # Lire tout le fichier
    type => "ratings"  # Type de données
  }
}

filter {
    # Utilisation de Grok pour extraire les champs du fichier
    grok {
      match => { "message" => "%{NUMBER:UserID}::%{NUMBER:MovieID}::%{NUMBER:Rating}::%{NUMBER:Timestamp}" }
      tag_on_failure => []
    }

    if "_grokparsefailure" in [tags] {
      drop { }
    }

    # Filtrer les événements avec des valeurs manquantes dans les champs obligatoires
    if [UserID] == "" or [MovieID] == "" or [Rating] == "" {
      drop { }
    }

    # Transformation des données (conversion des champs UserID, MovieID et Rating en entiers)
    mutate {
      convert => { "UserID" => "integer" }
      convert => { "MovieID" => "integer" }
      convert => { "Rating" => "integer" }
    }

    # Gestion des doublons en utilisant UserID et MovieID comme clé
    fingerprint {
      method => "MURMUR3"
      key => "%{UserID}-%{MovieID}"
      target => "[@metadata][fingerprint]"
    }
    if [message] not in [@metadata][fingerprint] {
      drop { }
    }
}

output {
  elasticsearch {
    hosts => "http://elasticsearch:9200"  # URL d'Elasticsearch
    index => "ratings"  # Nom de l'index cible dans Elasticsearch
    document_id => "%{UserID}-%{MovieID}"  # Utiliser une combinaison d'UserID et MovieID comme ID du document
  }
}
