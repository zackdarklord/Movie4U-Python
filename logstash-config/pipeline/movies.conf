
#Configuration pour l'ingestion de movies.dat

input {
  file {
    path => "/usr/share/logstash/pipeline/data_input/movies.dat"  # Chemin du fichier source
    start_position => "beginning"  # Commencer depuis le début du fichier
    sincedb_path => "/dev/null"  # Ignorer le fichier de suivi de position
    ignore_older => 0  # Lire tout le fichier
    type => "movies"  # Type de données
  }
}

filter {
    # Utilisation de Grok pour extraire les champs du fichier
    grok {
      match => { "message" => "%{NUMBER:MovieID}::%{DATA:Title}::%{DATA:Genres}" }
      tag_on_failure => []
    }

    if "_grokparsefailure" in [tags] {
      drop { }
    }

    # Filtrer les événements avec des valeurs manquantes dans les champs obligatoires
    if [MovieID] == "" or [Title] == "" {
      drop { }
    }

    # Filtrer les doublons en se basant sur le champ "MovieID"
    fingerprint {
      method => "MURMUR3"
      key => "%{MovieID}"
      target => "[@metadata][fingerprint]"
    }
    if [message] not in [@metadata][fingerprint] {
      drop { }
    }

    # Transformation des données (ajout d'un champ "Genres" sous forme de tableau)
    ruby {
      code => '
        genres = event.get("Genres").split("|")
        event.set("Genres", genres)
      '
    }

    # Transformation des données (conversion du champ "MovieID" en entier)
    mutate {
      convert => { "MovieID" => "integer" }
    }
}

output {
  elasticsearch {
    hosts => "http://elasticsearch:9200"  # URL d'Elasticsearch
    index => "movies"  # Nom de l'index cible dans Elasticsearch
    document_id => "%{MovieID}"  # Utiliser 'MovieID' comme ID du document
  }
}
