# Configuration pour l'ingestion de users.dat

input {
  file {
    path => "/usr/share/logstash/pipeline/data_input/users.dat"  # Chemin du fichier source
    start_position => "beginning"  # Commencer depuis le début du fichier
    sincedb_path => "/dev/null"  # Ignorer le fichier de suivi de position
    ignore_older => 0  # Lire tout le fichier
    type => "users"  # Type de données
  }
}

filter {
    # Utilisation de Grok pour extraire les champs du fichier
    grok {
      match => { "message" => "%{NUMBER:UserID}::%{DATA:Gender}::%{DATA:Age}::%{DATA:Occupation}::%{DATA:ZipCode}" }
      tag_on_failure => []
    }

    if "_grokparsefailure" in [tags] {
      drop { }
    }

    # Filtrer les événements avec des valeurs manquantes dans les champs obligatoires
    if [UserID] == "" or [Gender] == "" {
      drop { }
    }

    # Transformation des données (conversion du champ UserID en entier)
    mutate {
      convert => { "UserID" => "integer" }
    }

    # Gestion des doublons en utilisant UserID comme clé
    fingerprint {
      method => "MURMUR3"
      key => "%{UserID}"
      target => "[@metadata][fingerprint]"
    }
    if [message] not in [@metadata][fingerprint] {
      drop { }
    }
}

output {
  elasticsearch {
    hosts => "http://elasticsearch:9200"  # URL d'Elasticsearch
    index => "users"  # Nom de l'index cible dans Elasticsearch
    document_id => "%{UserID}"  # Utiliser UserID comme ID du document
  }
}
